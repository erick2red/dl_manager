#!/usr/bin/python2

import ConfigParser
import os
import stat
import sys
from time import sleep
import sqlite3
import pycurl
import re
from subprocess import *
from datetime import *
import signal

#### for now it will only download my urls

## global ##
global handlers, cparser, reload_configuration
handlers = []
cparser = ConfigParser.SafeConfigParser()
reload_configuration = False

def set_reload(signame, frame):
	global reload_configuration
	reload_configuration = True

def load_configuration():
	global cparser
	cfg = open('/home/erick/Sites/download_manager/cgi/dl_manager_service.cfg')
	cparser.readfp(cfg)
	cfg.close()

def get_handlers():
	rl = []
	conn = sqlite3.connect(cparser.get('Paths', 'database'))
	c = conn.cursor()
	### here it goes how to detect user_id
	user_id = 1
	c.execute('select handler,url_regex from handlers')
	for row in c:
		rl += [(row[0], re.compile(row[1]))]
	c.close()
	return rl

def fetch_urls(user):
	rl = []
	conn = sqlite3.connect(cparser.get('Paths', 'database'))
	c = conn.cursor()
	### here it goes how to detect user_id
	user_id = 1
	c.execute('select * from urls where user_id=' + str(user_id) + ' and status!=100')
	for row in c:
		#(1, 1, 0, u'http://localhost/~erick/phpinfo.php', u'phpinfo.php', 0, 0))
		# row[0] -> url_id
		# row[3] -> url
		# row[4] -> address
		# row[6] -> bw_limit
		rl += [(row[0], row[3], row[4], row[6])]
	c.close()
	return rl

def update_progress(url_id, url, progress, address):
	if progress == 100:
		#changing permissions of the dowloaded file
		os.chmod(address, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)
		size = os.stat(address).st_size / (1024*1024)
	else:
		size = 0
	conn = sqlite3.connect(cparser.get('Paths', 'database'))
	c = conn.cursor()
	size = os.stat(address).st_size / (1024*1024)
	c.execute('UPDATE urls SET status=' + str(progress) + ' ,address="' + address + '",size=' + str(size) + ' WHERE url_id=' + str(url_id))
	conn.commit()
	c.close()

def download(url_id, url, filename, bw_limit):
	dler = "default"
	for i in handlers:
		if i[1].match(url):
			dler = i[0]
			break
	if dler == 'youtube-dl':
		os.chdir(cparser.get('Paths', 'download_files'))
		try:
			retcode = call('youtube-dl ' + url, shell=True)
			if retcode != 0:
				print('Child "youtube-dl ' + url + '" was terminated by with return code ', -retcode)
				return None
			else:
				p = Popen('youtube-dl ' + url + ' --get-filename', stdout=PIPE, shell=True)
				sts = os.waitpid(p.pid, 0)[1]
				output = p.stdout.readlines()
				filename = output[0].decode('utf_8')[:-1]
				dfile_name = os.path.join(cparser.get('Paths', 'download_files'), filename)
				return dfile_name
		except OSError as e:
			print("Execution failed:", e)
	else:
		try:
			dfile_name = os.path.join(cparser.get('Paths', 'download_files'), filename)

			fp = open(dfile_name + '.part', "wb")
			curl = pycurl.Curl()
			curl.setopt(pycurl.FOLLOWLOCATION, 1)
			curl.setopt(pycurl.MAXREDIRS, 5)
			curl.setopt(pycurl.CONNECTTIMEOUT, 30)
			curl.setopt(pycurl.TIMEOUT, 300)
			if bw_limit != 0:
				curl.setopt(pycurl.MAX_RECV_SPEED_LARGE, bw_limit)
			elif cparser.getint('Internals', 'max_average_speed') != 0:
				curl.setopt(pycurl.MAX_RECV_SPEED_LARGE, cparser.getint('Internals', 'max_average_speed'))
			curl.setopt(pycurl.WRITEDATA, fp)
			curl.setopt(pycurl.URL, str(url))
			try:
				curl.perform()
			except:
				import traceback
				traceback.print_exc(file=sys.stderr)
				sys.stderr.flush()
			curl.close()
			fp.close()
			os.rename(dfile_name + '.part', dfile_name)
			return dfile_name
		except Exception as e:
			print('Error: ' + e.__str__())
			print('url: ' + url)
			print('pycurl.URL: ' + pycurl.URL.__str__())
			if os.path.exists(dfile_name + '.part'):
				os.remove(dfile_name + '.part')
			return None

if __name__ == '__main__':
	signal.signal(signal.SIGUSR1, set_reload)
	load_configuration()
	handlers = get_handlers()
	while True:
		if reload_configuration:
			load_configuration()
			reload_configuration = False
		if datetime.now().time() > time(int(cparser.get('Internals', 'start_hour').split(':')[0]), int(cparser.get('Internals', 'start_hour').split(':')[1])) or datetime.now().time() < time(int(cparser.get('Internals', 'end_hour').split(':')[0]), int(cparser.get('Internals', 'end_hour').split(':')[1])):
			urls = fetch_urls('erick')
			for i in urls:
				print('Got url: ' + i[1])
				returned_value = download(i[0], i[1], i[2], i[3])
				if returned_value != None:
					#success
					 update_progress(i[0], i[1], 100, returned_value)
			print('Waiting for urls')
			sleep(2)
		else:
			print('Waiting for hours')
			sleep(300)
			

