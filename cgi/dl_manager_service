#!/usr/bin/python2

#### for now it will only download my urls

#### configuration ####
start_hour = 16
start_min = 9
database_path = '/home/erick/Sites/download_manager/data/db.sqlite'
download_files_path = "/home/erick/Sites/download_manager/dl_files"
max_average_speed = 0 #bytes
handlers = []

import os
import stat
import sys
from time import sleep
import sqlite3
import pycurl
import re
from subprocess import *

def get_handlers():
	rl = []
	conn = sqlite3.connect(database_path)
	c = conn.cursor()
	### here it goes how to detect user_id
	user_id = 1
	c.execute('select handler,url_regex from handlers')
	for row in c:
		rl += [(row[0], re.compile(row[1]))]
	c.close()
	return rl

def fetch_urls(user):
	rl = []
	conn = sqlite3.connect(database_path)
	c = conn.cursor()
	### here it goes how to detect user_id
	user_id = 1
	c.execute('select * from urls where user_id=' + str(user_id) + ' and status!=100')
	for row in c:
		#(1, 1, 0, u'http://localhost/~erick/phpinfo.php', u'phpinfo.php', 0, 0))
		# row[0] -> url_id
		# row[3] -> url
		# row[4] -> address
		# row[6] -> bw_limit
		rl += [(row[0], row[3], row[4], row[6])]
	c.close()
	return rl

def update_progress(url_id, url, progress, address):
	if progress == 100:
		#changing permissions of the dowloaded file
		os.chmod(address, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)
		size = os.stat(address).st_size / (1024*1024)
	else:
		size = 0
	conn = sqlite3.connect(database_path)
	c = conn.cursor()
	size = os.stat(address).st_size / (1024*1024)
	c.execute('UPDATE urls SET status=' + str(progress) + ' ,address="' + address + '",size=' + str(size) + ' WHERE url_id=' + str(url_id))
	conn.commit()
	c.close()

def download(url_id, url, filename, bw_limit):
	dler = "default"
	for i in handlers:
		if i[1].match(url):
			dler = i[0]
			break
	if dler == 'youtube-dl':
		os.chdir(download_files_path)
		try:
			retcode = call('youtube-dl ' + url, shell=True)
			if retcode != 0:
				print('Child "youtube-dl ' + url + '" was terminated by with return code ', -retcode)
				return None
			else:
				p = Popen('youtube-dl ' + url + ' --get-filename', stdout=PIPE, shell=True)
				sts = os.waitpid(p.pid, 0)[1]
				output = p.stdout.readlines()
				filename = output[0].decode('utf_8')[:-1]
				dfile_name = os.path.join(download_files_path, filename)
				return dfile_name
		except OSError as e:
			print("Execution failed:", e)
	else:
		try:
			dfile_name = os.path.join(download_files_path, filename)

			fp = open(dfile_name + '.part', "wb")
			curl = pycurl.Curl()
			curl.setopt(pycurl.FOLLOWLOCATION, 1)
			curl.setopt(pycurl.MAXREDIRS, 5)
			curl.setopt(pycurl.CONNECTTIMEOUT, 30)
			curl.setopt(pycurl.TIMEOUT, 300)
			if bw_limit != 0:
				curl.setopt(pycurl.MAX_RECV_SPEED_LARGE, bw_limit)
			elif max_average_speed != 0:
				curl.setopt(pycurl.MAX_RECV_SPEED_LARGE, max_average_speed)
			curl.setopt(pycurl.WRITEDATA, fp)
			curl.setopt(pycurl.URL, str(url))
			try:
				curl.perform()
			except:
				import traceback
				traceback.print_exc(file=sys.stderr)
				sys.stderr.flush()
			curl.close()
			fp.close()
			os.rename(dfile_name + '.part', dfile_name)
			return dfile_name
		except Exception as e:
			print('Error: ' + e.__str__())
			print('url: ' + url)
			print('pycurl.URL: ' + pycurl.URL.__str__())
			if os.path.exists(dfile_name + '.part'):
				os.remove(dfile_name + '.part')
			return None

if __name__ == '__main__':
	handlers = get_handlers()
	while True:
		urls = fetch_urls('erick')
		for i in urls:
			print('Got url: ' + i[1])
			returned_value = download(i[0], i[1], i[2], i[3])
			if returned_value != None:
				#success
				 update_progress(i[0], i[1], 100, returned_value)
		print('Waiting for urls')
		sleep(30)

