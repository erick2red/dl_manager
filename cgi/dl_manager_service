#!/usr/bin/python

#### for now it will only download my urls

#### configuration ####
start_hour = 16
start_min = 9
database_path = '/home/erick/Sites/dl_manager/data/db.sqlite'
download_files_path = "/home/erick/Sites/dl_manager/dl_files"
log_file = '/var/log/dl_manager_service.log'

import os
from time import sleep
import sqlite3
import urllib.request

def fetch_urls(user):
	rl = []
	conn = sqlite3.connect(database_path)
	c = conn.cursor()
	### here it goes how to detect user_id
	user_id = 1
	c.execute('select * from urls where user_id=' + str(user_id))
	for row in c:
		if row[2] == 'undone':
			rl += [(row[0], row[3], row[4])]
	c.close()
	return rl

def set_done(url_id, address):
	conn = sqlite3.connect(database_path)
	c = conn.cursor()
	size = os.stat(address).st_size / (1024*1024)
	c.execute('UPDATE urls SET status="done",address="' + address + '",size=' + str(size) + ' WHERE url_id=' + str(url_id))
	conn.commit()
	c.close()

def download(url, filename):
	try:
		dfile_name = os.path.join(download_files_path, filename)
		dfile = urllib.request.urlretrieve(url, dfile_name + '.part')
		os.rename(dfile_name + '.part', dfile_name)
		return dfile_name
	except Exception as e:
		os.remove(dfile_name + '.part')
		return None

if __name__ == '__main__':
	while True:
		urls = fetch_urls('erick')
		print(urls)
		for i in urls:
			returned_value = download(i[1], i[2])
			if returned_value != None:
				#success
				set_done(i[0], returned_value)
		sleep(300)

	
